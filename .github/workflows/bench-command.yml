name: Benchmark Command
# Trigger on-demand benchmarks via PR comments
# Usage: /bench <ref1> <ref2> [iterations] [sizes]
# Examples:
#   /bench main v0.13.0
#   /bench abc123 def456 100 1000,5000,10000
# Only repository owner can trigger this command

on:
  issue_comment:
    types: [created]

# Prevent concurrent benchmark runs on the same PR
concurrency:
  group: bench-${{ github.event.issue.number }}
  cancel-in-progress: false

jobs:
  check-permission:
    name: Check Command Permission
    # Only run on PR comments (not regular issues)
    if: |
      github.event.issue.pull_request &&
      startsWith(github.event.comment.body, '/bench ')
    runs-on: ubuntu-latest
    permissions:
      issues: write
      pull-requests: write
    outputs:
      authorized: ${{ steps.check.outputs.authorized }}
      ref1: ${{ steps.parse.outputs.ref1 }}
      ref2: ${{ steps.parse.outputs.ref2 }}
      iterations: ${{ steps.parse.outputs.iterations }}
      sizes: ${{ steps.parse.outputs.sizes }}
    steps:
      - name: Check if commenter is repo owner
        id: check
        uses: actions/github-script@v7
        with:
          script: |
            const commenter = context.payload.comment.user.login;
            const owner = context.payload.repository.owner.login;
            const isOwner = commenter === owner;

            console.log(`Commenter: ${commenter}`);
            console.log(`Repository owner: ${owner}`);
            console.log(`Is owner: ${isOwner}`);

            if (!isOwner) {
              await github.rest.reactions.createForIssueComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: context.payload.comment.id,
                content: '-1'
              });

              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: `‚ùå **Permission denied**: Only @${owner} can trigger benchmark comparisons.`
              });
            } else {
              await github.rest.reactions.createForIssueComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: context.payload.comment.id,
                content: 'eyes'
              });
            }

            core.setOutput('authorized', isOwner);

      - name: Parse benchmark command
        id: parse
        if: steps.check.outputs.authorized == 'true'
        continue-on-error: true
        run: |
          set -euo pipefail
          COMMENT="${{ github.event.comment.body }}"

          # Parse command: /bench ref1 ref2 [iterations] [sizes]
          # Remove /bench prefix and extract parameters
          PARAMS=$(echo "$COMMENT" | sed 's|^/bench[[:space:]]*||')

          # Extract parameters
          REF1=$(echo "$PARAMS" | awk '{print $1}')
          REF2=$(echo "$PARAMS" | awk '{print $2}')
          ITERATIONS=$(echo "$PARAMS" | awk '{print $3}')
          SIZES=$(echo "$PARAMS" | awk '{print $4}')

          # Validate required parameters
          if [ -z "$REF1" ] || [ -z "$REF2" ]; then
            echo "error=Invalid format. Missing required parameters." >> $GITHUB_OUTPUT
            echo "parse_failed=true" >> $GITHUB_OUTPUT
            exit 1
          fi

          # Set defaults for optional parameters
          if [ -z "$ITERATIONS" ] || ! [[ "$ITERATIONS" =~ ^[0-9]+$ ]]; then
            ITERATIONS=100
          fi

          if [ -z "$SIZES" ]; then
            SIZES="1000,5000,10000"
          fi

          # Validate sizes format (comma-separated numbers)
          if ! echo "$SIZES" | grep -qE '^[0-9]+(,[0-9]+)*$'; then
            echo "error=Invalid sizes format: $SIZES" >> $GITHUB_OUTPUT
            echo "parse_failed=true" >> $GITHUB_OUTPUT
            exit 1
          fi

          echo "ref1=$REF1" >> $GITHUB_OUTPUT
          echo "ref2=$REF2" >> $GITHUB_OUTPUT
          echo "iterations=$ITERATIONS" >> $GITHUB_OUTPUT
          echo "sizes=$SIZES" >> $GITHUB_OUTPUT
          echo "parse_failed=false" >> $GITHUB_OUTPUT

          echo "Parsed parameters:"
          echo "  ref1: $REF1"
          echo "  ref2: $REF2"
          echo "  iterations: $ITERATIONS"
          echo "  sizes: $SIZES"

      - name: Post parse error
        if: steps.check.outputs.authorized == 'true' && steps.parse.outcome == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.reactions.createForIssueComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: context.payload.comment.id,
              content: 'confused'
            });

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: `‚ùå **Invalid command format**

**Usage:** \`/bench <ref1> <ref2> [iterations] [sizes]\`

**Examples:**
\`\`\`
/bench main v0.13.0
/bench abc123 def456 100 1000,5000,10000
\`\`\`

**Parameters:**
- \`ref1\` (required): Baseline git reference
- \`ref2\` (required): Current git reference
- \`iterations\` (optional): Number of iterations (default: 100)
- \`sizes\` (optional): Comma-separated sizes (default: 1000,5000,10000)`
            });

      - name: Post acknowledgment
        if: steps.check.outputs.authorized == 'true' && steps.parse.outcome == 'success'
        uses: actions/github-script@v7
        with:
          script: |
            const ref1 = '${{ steps.parse.outputs.ref1 }}';
            const ref2 = '${{ steps.parse.outputs.ref2 }}';
            const iterations = '${{ steps.parse.outputs.iterations }}';
            const sizes = '${{ steps.parse.outputs.sizes }}';

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: `üöÄ **Benchmark comparison started**

**Comparing:**
- **Baseline**: \`${ref1}\`
- **Current**: \`${ref2}\`

**Parameters:**
- **Iterations**: ${iterations}
- **Sizes**: ${sizes}

Results will be posted here when complete...`
            });

  run-benchmarks:
    name: Run Benchmark Comparison
    needs: check-permission
    if: needs.check-permission.outputs.authorized == 'true' && needs.check-permission.outputs.ref1 != ''
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
      issues: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history to access all refs

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Rust dependencies
        uses: Swatinem/rust-cache@v2

      - name: Fetch refs from remote
        run: |
          set -euo pipefail
          REF1="${{ needs.check-permission.outputs.ref1 }}"
          REF2="${{ needs.check-permission.outputs.ref2 }}"

          echo "Fetching ref1: $REF1"
          git fetch origin "$REF1" || git fetch origin "refs/tags/$REF1" || git fetch origin "refs/heads/$REF1" || true

          echo "Fetching ref2: $REF2"
          git fetch origin "$REF2" || git fetch origin "refs/tags/$REF2" || git fetch origin "refs/heads/$REF2" || true

          # Update remote refs
          git fetch origin --tags

      - name: Validate refs exist
        id: validate
        run: |
          set -euo pipefail
          REF1="${{ needs.check-permission.outputs.ref1 }}"
          REF2="${{ needs.check-permission.outputs.ref2 }}"

          if ! git rev-parse --verify "$REF1" >/dev/null 2>&1; then
            echo "error=Ref '$REF1' not found" >> $GITHUB_OUTPUT
            exit 1
          fi

          if ! git rev-parse --verify "$REF2" >/dev/null 2>&1; then
            echo "error=Ref '$REF2' not found" >> $GITHUB_OUTPUT
            exit 1
          fi

          echo "ref1_sha=$(git rev-parse --short $REF1)" >> $GITHUB_OUTPUT
          echo "ref2_sha=$(git rev-parse --short $REF2)" >> $GITHUB_OUTPUT

      - name: Check benchmark tool exists in ref1
        id: check_ref1_tool
        run: |
          set -euo pipefail
          REF1="${{ needs.check-permission.outputs.ref1 }}"
          echo "Checking out $REF1..."
          git checkout "$REF1"

          # Check if bench_throughput binary is defined in Cargo.toml
          if ! grep -q 'name = "bench_throughput"' Cargo.toml 2>/dev/null; then
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "‚ùå Benchmark tool 'bench_throughput' not found in $REF1"
            exit 0
          fi

          # Check if the source file exists
          if ! grep -A 2 'name = "bench_throughput"' Cargo.toml | grep -q 'path.*='; then
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "‚ùå Benchmark tool source file not found in $REF1"
            exit 0
          fi

          echo "exists=true" >> $GITHUB_OUTPUT
          echo "‚úì Benchmark tool found in $REF1"

      - name: Check benchmark tool exists in ref2
        id: check_ref2_tool
        run: |
          set -euo pipefail
          REF2="${{ needs.check-permission.outputs.ref2 }}"
          echo "Checking out $REF2..."
          git checkout "$REF2"

          # Check if bench_throughput binary is defined in Cargo.toml
          if ! grep -q 'name = "bench_throughput"' Cargo.toml 2>/dev/null; then
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "‚ùå Benchmark tool 'bench_throughput' not found in $REF2"
            exit 0
          fi

          # Check if the source file exists
          if ! grep -A 2 'name = "bench_throughput"' Cargo.toml | grep -q 'path.*='; then
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "‚ùå Benchmark tool source file not found in $REF2"
            exit 0
          fi

          echo "exists=true" >> $GITHUB_OUTPUT
          echo "‚úì Benchmark tool found in $REF2"

      - name: Post missing tool error
        if: steps.check_ref1_tool.outputs.exists == 'false' || steps.check_ref2_tool.outputs.exists == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            const ref1 = '${{ needs.check-permission.outputs.ref1 }}';
            const ref2 = '${{ needs.check-permission.outputs.ref2 }}';
            const ref1_exists = '${{ steps.check_ref1_tool.outputs.exists }}' === 'true';
            const ref2_exists = '${{ steps.check_ref2_tool.outputs.exists }}' === 'true';

            let message = '‚ùå **Benchmark comparison failed**\n\n';
            message += '**Reason**: The benchmark tool (`bench_throughput`) does not exist in ';

            if (!ref1_exists && !ref2_exists) {
              message += `both refs:\n- \`${ref1}\`\n- \`${ref2}\``;
            } else if (!ref1_exists) {
              message += `ref: \`${ref1}\``;
            } else {
              message += `ref: \`${ref2}\``;
            }

            message += '\n\n**Solution**: The benchmark tool was added in commit `d264124`. Please use refs that include this commit or later.';
            message += '\n\n**Example**: `/bench main HEAD` (if both include the benchmark tool)';

            await github.rest.reactions.createForIssueComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: ${{ github.event.comment.id }},
              content: 'confused'
            });

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: message
            });

            // Exit workflow gracefully
            process.exit(78);  // Neutral exit code

      - name: Benchmark ref1 (baseline)
        if: steps.check_ref1_tool.outputs.exists == 'true' && steps.check_ref2_tool.outputs.exists == 'true'
        run: |
          set -euo pipefail
          REF1="${{ needs.check-permission.outputs.ref1 }}"
          ITERATIONS="${{ needs.check-permission.outputs.iterations }}"
          SIZES="${{ needs.check-permission.outputs.sizes }}"

          echo "Checking out $REF1..."
          git checkout "$REF1"

          echo "Building benchmark tool..."
          if ! cargo build --release --bin bench_throughput 2>&1 | tee build_ref1.log; then
            echo "‚ùå Failed to build benchmark tool for $REF1"
            exit 1
          fi

          echo "Running benchmarks..."
          ./target/release/bench_throughput \
            --sizes "$SIZES" \
            --iterations "$ITERATIONS" \
            --format json \
            --output benchmark_ref1.json

      - name: Benchmark ref2 (current)
        if: steps.check_ref1_tool.outputs.exists == 'true' && steps.check_ref2_tool.outputs.exists == 'true'
        run: |
          set -euo pipefail
          REF2="${{ needs.check-permission.outputs.ref2 }}"
          ITERATIONS="${{ needs.check-permission.outputs.iterations }}"
          SIZES="${{ needs.check-permission.outputs.sizes }}"

          echo "Checking out $REF2..."
          git checkout "$REF2"

          # Rebuild in case dependencies changed
          echo "Building benchmark tool..."
          if ! cargo build --release --bin bench_throughput 2>&1 | tee build_ref2.log; then
            echo "‚ùå Failed to build benchmark tool for $REF2"
            exit 1
          fi

          echo "Running benchmarks..."
          ./target/release/bench_throughput \
            --sizes "$SIZES" \
            --iterations "$ITERATIONS" \
            --format json \
            --output benchmark_ref2.json

      - name: Compare results
        if: steps.check_ref1_tool.outputs.exists == 'true' && steps.check_ref2_tool.outputs.exists == 'true'
        run: |
          set -euo pipefail
          # Use the comparison script from ref2 (current)
          if [ -f scripts/compare_benchmarks.py ]; then
            python3 scripts/compare_benchmarks.py \
              benchmark_ref1.json \
              benchmark_ref2.json > comparison.md
          else
            echo "‚ùå Comparison script not found"
            exit 1
          fi

      - name: Post results to PR
        if: steps.check_ref1_tool.outputs.exists == 'true' && steps.check_ref2_tool.outputs.exists == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comparison = fs.readFileSync('comparison.md', 'utf8');
            const ref1 = '${{ needs.check-permission.outputs.ref1 }}';
            const ref2 = '${{ needs.check-permission.outputs.ref2 }}';
            const ref1_sha = '${{ steps.validate.outputs.ref1_sha }}';
            const ref2_sha = '${{ steps.validate.outputs.ref2_sha }}';
            const iterations = '${{ needs.check-permission.outputs.iterations }}';
            const sizes = '${{ needs.check-permission.outputs.sizes }}';

            const body = `## üî¨ Benchmark Comparison Report

**Requested by:** @${{ github.event.comment.user.login }}

**Comparison:**
- **Baseline**: \`${ref1}\` (${ref1_sha})
- **Current**: \`${ref2}\` (${ref2_sha})

**Parameters:**
- **Iterations**: ${iterations}
- **Sizes**: ${sizes}

---

${comparison}

---

<sub>Triggered by [/bench command](${{ github.event.comment.html_url }})</sub>`;

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });

      - name: Upload benchmark artifacts
        if: steps.check_ref1_tool.outputs.exists == 'true' && steps.check_ref2_tool.outputs.exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-comparison-${{ github.event.comment.id }}
          path: |
            benchmark_ref1.json
            benchmark_ref2.json
            comparison.md
            build_ref1.log
            build_ref2.log
          retention-days: 30

      - name: Add success reaction
        if: steps.check_ref1_tool.outputs.exists == 'true' && steps.check_ref2_tool.outputs.exists == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.reactions.createForIssueComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: ${{ github.event.comment.id }},
              content: 'rocket'
            });

  handle-error:
    name: Handle Errors
    needs: [check-permission, run-benchmarks]
    if: failure() && needs.check-permission.outputs.authorized == 'true'
    runs-on: ubuntu-latest
    permissions:
      issues: write
    steps:
      - name: Post error message
        uses: actions/github-script@v7
        with:
          script: |
            const ref1 = '${{ needs.check-permission.outputs.ref1 }}';
            const ref2 = '${{ needs.check-permission.outputs.ref2 }}';

            await github.rest.reactions.createForIssueComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: ${{ github.event.comment.id }},
              content: 'confused'
            });

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: `‚ùå **Benchmark comparison failed**

Failed to compare \`${ref1}\` and \`${ref2}\`.

**Please check:**
- Both refs exist and are valid git references (branches, tags, or commits)
- The benchmark tool exists in both refs
- The code at those refs compiles successfully
- Parameters are in correct format: \`/bench <ref1> <ref2> [iterations] [sizes]\`

**See the [workflow run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) for details.**`
            });
